# metadata.yaml for BERT CPU benchmark
name: bert_cpu
type: macrobenchmark
language: Python
domain:
  - natural_language_processing
  - transformer_models
  - deep_learning

setup:
  command: python3 setup.py

workloads:
  - name: bert_eval
    summary: BERT deep learning inference benchmark on CPU
    action: execute forward pass inference with BERT model
    description: >
      Load pre-generated input batches and initialize the BERT model in evaluation mode.
      Execute the forward pass on the CPU to process input sequences without gradient calculation.
      Measure the inference latency to assess the CPU's performance on Transformer-based NLP workloads.
    data:
      command: python3 generate_data.py
      parameters:
        batch_size:
          description: Batch size for each batch
          default: 16
        seq_len:
          description: Sequence length for each input
          default: 1024
        num_batches:
          description: Number of batches to generate
          default: 50
        output_dir:
          description: Directory to save generated batches
          default: ./data
    command: python3 bert_eval.py
    parameters:
      data_dir:
        description: Directory containing pre-generated input batches
        default: ./data
      threads:
        description: Number of CPU threads to use for training
        default: 1
      lr:
        description: Learning rate for optimizer
        default: 0.001
      compile:
        description: Use torch.compile (PyTorch 2.x only)
        default: false

characteristics:
  IO_effects: low
  single_machine: true
  network_effects: none
  cpu_bound: true
  memory_intensive: low

tags:
  - transformer
  - NLP
  - deep_learning
  - training

dependencies:
  python: ">=3.8"
  torch: ">=2.0"
  transformers: ">=4.30"
