# metadata.yaml for BERT CPU benchmark
name: bert_cpu
type: macrobenchmark
language: Python
domain:
  - natural_language_processing
  - transformer_models
  - deep_learning

setup:
  command: python3 setup.py

workloads:
  - name: bert_eval
    description: >
      Execute BERT CPU evaluation using pre-generated input batches.
      Measures the performance of forward pass on CPU.
    data:
      command: python3 generate_data.py
      parameters:
        batch_size:
          description: Batch size for each batch
          default: 16
        seq_len:
          description: Sequence length for each input
          default: 1024
        num_batches:
          description: Number of batches to generate
          default: 50
        output_dir:
          description: Directory to save generated batches
          default: ./data
    command: python3 bert_eval.py
    parameters:
      data_dir:
        description: Directory containing pre-generated input batches
        default: ./data
      threads:
        description: Number of CPU threads to use for training
        default: 1
      lr:
        description: Learning rate for optimizer
        default: 0.001
      compile:
        description: Use torch.compile (PyTorch 2.x only)
        default: false

characteristics:
  IO_effects: low
  single_machine: true
  network_effects: none
  cpu_bound: true
  memory_intensive: low

tags:
  - transformer
  - NLP
  - deep_learning
  - training

dependencies:
  python: ">=3.8"
  torch: ">=2.0"
  transformers: ">=4.30"
