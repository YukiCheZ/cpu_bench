# metadata.yaml for Full Transformer CPU Inference Benchmark
name: transformer_inference
type: macrobenchmark
language: Python
domain:
  - deep_learning
  - transformers
  - natural_language_processing

setup:
  command: python3 setup.py

workloads:
  - name: transformer_inference
    description: >
      Execute the full Transformer (Encoder + Decoder) CPU inference benchmark. 
      This workload measures the performance of multi-head self-attention, 
      feedforward networks, and sequence processing on CPU.
    data: 
      command: python3 generate_data.py
      parameters:
        output_dir:
          description: Directory to save generated batches
          default: ./data
        num_batches:
          description: Number of batches to generate
          default: 50
        batch_size:
          description: Number of sequences per batch
          default: 8
        seq_len:
          description: Sequence length
          default: 256
        vocab_size:
          description: Vocabulary size for input tokens
          default: 10000
    command: python3 run_benchmark.py
    parameters:
      data_dir:
        description: Directory containing pre-generated batches and metadata.json
        default: ./data
      iters:
        description: Number of iterations to run inference
        default: 1
      threads:
        description: Number of CPU threads to use
        default: 1
      num_encoder_layers:
        description: Number of encoder layers in Transformer
        default: 24
      num_decoder_layers:
        description: Number of decoder layers in Transformer
        default: 24
      d_model:
        description: Transformer embedding dimension
        default: 1024
      nhead:
        description: Number of attention heads
        default: 32
      dim_feedforward:
        description: Feedforward hidden size in each Transformer layer
        default: 4096
      compile:
        description: Use torch.compile (PyTorch 2.x only)
        default: false

characteristics:
  IO_effects: low
  single_machine: true
  network_effects: none
  cpu_bound: true
  memory_intensive: medium

tags:
  - transformer
  - deep_learning
  - encoder_decoder
  - attention

dependencies:
  python: ">=3.8"
  torch: ">=2.0"
