# metadata.yaml for Full Transformer CPU Inference Benchmark
name: transformer_inference
type: macrobenchmark
language: Python
domain:
  - deep_learning
  - transformers
  - natural_language_processing

setup:
  command: python3 setup.py

workloads:
  - name: transformer_inference
    summary: Full Transformer Encoder-Decoder inference benchmark
    action: execute sequence-to-sequence inference with Transformer model
    description: >
      Initialize a full Transformer model comprising both Encoder and Decoder stacks with configurable layers and attention heads.
      Execute the forward pass on pre-generated sequence batches to simulate machine translation or sequence generation tasks.
      Measure the computational throughput of multi-head self-attention mechanisms and feedforward networks to evaluate CPU performance for NLP workloads.
    data: 
      command: python3 generate_data.py
      parameters:
        output_dir:
          full_name: output directory
          description: Directory to save generated batches
          default: ./data
        num_batches:
          full_name: number of batches
          description: Number of batches to generate
          default: 50
        batch_size:
          full_name: batch size
          description: Number of sequences per batch
          default: 8
        seq_len:
          full_name: sequence length
          description: Sequence length
          default: 256
        vocab_size:
          full_name: vocabulary size
          description: Vocabulary size for input tokens
          default: 10000
    command: python3 run_benchmark.py
    parameters:
      data_dir:
        full_name: data directory
        description: Directory containing pre-generated batches and metadata.json
        default: ./data
      iters:
        full_name: iterations
        description: Number of iterations to run inference
        default: 1
      threads:
        full_name: threads
        description: Number of CPU threads to use
        default: 1
      num_encoder_layers:
        full_name: number of encoder layers
        description: Number of encoder layers in Transformer
        default: 24
      num_decoder_layers:
        full_name: number of decoder layers
        description: Number of decoder layers in Transformer
        default: 24
      d_model:
        full_name: embedding dimension
        description: Transformer embedding dimension
        default: 1024
      nhead:
        full_name: number of attention heads
        description: Number of attention heads
        default: 32
      dim_feedforward:
        full_name: feedforward dimension
        description: Feedforward hidden size in each Transformer layer
        default: 4096
      compile:
        full_name: compile
        description: Use torch.compile
        default: false

characteristics:
  IO_effects: low
  single_machine: true
  network_effects: none
  cpu_bound: true
  memory_intensive: medium

tags:
  - transformer
  - deep_learning
  - encoder_decoder
  - attention

dependencies:
  python: ">=3.8"
  torch: ">=2.0"
